import numpy as np
import gym
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM
from tensorflow.keras.optimizers import Adam
import pandas as pd
from tensorflow.keras.optimizers import Adam
import tensorflow as tf

# Add the 'price_data_window' variable to determine the number of historical prices for each episode
price_data_window = 100

class LSTMPPOAgentWithIndicators:
    def __init__(self, input_dim, output_dim, indicator_dim):
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.indicator_dim = indicator_dim
        self.model = self._build_model()
        self.old_model = self._build_model()  # For PPO
        self.model.set_weights(self.old_model.get_weights())  # Initialize old model with the same weights
        self.memory = []  # Memory to store experience tuples
        self.gamma = 0.99
        self.clip_ratio = 0.2
        self.batch_size = 32
        self.epochs = 10

    def _build_model(self):
        model = Sequential()
        model.add(LSTM(64, input_shape=(None, self.input_dim + self.indicator_dim)))
        model.add(Dense(self.output_dim, activation='softmax'))
        model.compile(loss=self.ppo_loss, optimizer=Adam(learning_rate=0.01))  # Set the learning rate in Adam optimizer
        return model
    
    def ppo_loss(self, y_true, y_pred):
        advantages, old_probs = y_true[:, :1], y_true[:, 1:]
        new_probs = y_pred

        ratio = new_probs / (old_probs + 1e-5)
        clip_advantages = np.clip(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages
        min_advantages = np.minimum(advantages, clip_advantages)

        entropy = -new_probs * np.log(new_probs + 1e-5)

        return -np.mean(min_advantages + 0.01 * entropy)

    def remember(self, state, action, reward, next_state, done, prob):
        self.memory.append((state, action, reward, next_state, done, prob))

  def act(self, state):
        state = np.expand_dims(state, axis=0)
        state_with_indicators = state.astype(np.float32)  # Convert the data type to float32
        state_tensor = tf.constant(state_with_indicators, dtype=tf.float32)
        action_probs = self.model.predict(state_tensor)[0]
        return np.random.choice(np.arange(self.output_dim), p=action_probs), action_probs


    def replay(self):
        if len(self.memory) < self.batch_size:
            return

        states, actions, rewards, next_states, dones, old_probs = [], [], [], [], [], []
        for state, action, reward, next_state, done, prob in self.memory:
            states.append(state)
            actions.append(action)
            rewards.append(reward)
            next_states.append(next_state)
            dones.append(done)
            old_probs.append(prob)

        states = np.array(states)
        actions = np.array(actions)
        rewards = np.array(rewards)
        next_states = np.array(next_states)
        dones = np.array(dones)
        old_probs = np.array(old_probs)

        next_values = self.model.predict(next_states)
        next_values = np.squeeze(next_values)

        values = self.model.predict(states)
        values = np.squeeze(values)

        advantages = rewards + self.gamma * (1 - dones) * next_values - values
        advantages = np.expand_dims(advantages, axis=-1)

        labels = np.hstack([advantages, old_probs])

        for _ in range(self.epochs):
            self.model.fit(states, labels, batch_size=self.batch_size, shuffle=True, epochs=1, verbose=0)

        self.old_model.set_weights(self.model.get_weights())
        
# Function to calculate RSI
def calculate_rsi(prices, window=14):
    deltas = np.diff(prices)
    gain = np.where(deltas > 0, deltas, 0)
    loss = np.where(deltas < 0, -deltas, 0)
    avg_gain = np.mean(gain[:window])
    avg_loss = np.mean(loss[:window])
    rs = avg_gain / (avg_loss + 1e-5)
    rsi = 100 - (100 / (1 + rs))
    return rsi

# Function to calculate MACD
def calculate_macd(prices, short_window=12, long_window=26, signal_window=9):
    short_ema = np.mean(prices[-short_window:])
    long_ema = np.mean(prices[-long_window:])
    macd = short_ema - long_ema
    macd_values = np.array([macd])  # Store the initial MACD value in an array

    for price in prices[-(long_window + 1):-1]:  # Iterate over the historical prices to calculate MACD
        short_ema = (price - short_ema) * (2 / (short_window + 1)) + short_ema
        long_ema = (price - long_ema) * (2 / (long_window + 1)) + long_ema
        macd = short_ema - long_ema
        macd_values = np.append(macd_values, macd)  # Append the new MACD value to the array

    signal_line = np.mean(macd_values[-signal_window:])
    return macd_values, signal_line


def train_lstm_ppo_agent_with_indicators():
    env = gym.make('CartPole-v1')
    input_dim = env.observation_space.shape[0]
    output_dim = env.action_space.n
    indicator_dim = 3  # RSI and MACD will add 2 extra indicators

    agent = LSTMPPOAgentWithIndicators(input_dim, output_dim, indicator_dim)
    episodes = 1000

    # Read historical price data from the CSV file
    price_data_df = pd.read_csv("12.csv")
    price_data = price_data_df["close"].values  # Assuming the CSV file has a "Close" column

    for episode in range(episodes):
        rsi = calculate_rsi(price_data[-price_data_window:])  # Calculate RSI using the last 'price_data_window' elements
        macd_values, signal_line = calculate_macd(price_data[-price_data_window:])  # Calculate MACD using the last 'price_data_window' elements

        state = env.reset()
        done = False
        total_reward = 0
        
        while not done:
            # Extract indicators from RSI and MACD
            indicators = np.array([rsi, macd_values[-1], signal_line])  # Use the latest MACD value

            # Concatenate indicators to the state
            state_with_indicators = np.concatenate([state, indicators]).astype(np.float32)  # Convert to float32
            action, action_probs = agent.act(state_with_indicators)
            next_state, reward, done, _ = env.step(action)

            # Extract indicators for the next state
            next_rsi = calculate_rsi(np.append(price_data[-price_data_window:], next_state[-1]))  # Update the RSI with the new price
            next_macd_values, next_signal_line = calculate_macd(np.append(price_data[-price_data_window:], next_state[-1]))  # Update the MACD with the new price

            next_indicators = np.array([next_rsi, next_macd_values[-1], next_signal_line], dtype=np.float32)  # Convert to float32
            next_state_with_indicators = np.concatenate([next_state, next_indicators])

            agent.remember(state_with_indicators, action, reward, next_state_with_indicators, done, action_probs[action])
            total_reward += reward
            state = next_state

        agent.replay()

        print(f"Episode {episode + 1} - Total Reward: {total_reward}")

if __name__ == "__main__":
    train_lstm_ppo_agent_with_indicators()
